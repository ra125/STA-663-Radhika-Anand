{
 "metadata": {
  "name": "",
  "signature": "sha256:e1d151f064772d8ae3ec9deb50b5ad8961583b06c3da1531618b17b9cfbd8309"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as la\n",
      "import scipy.stats as st"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Latent Semantic Analysis (LSA) is a method for reducing the dimnesionality of documents treated as a bag of words. It is used for document classification, clustering and retrieval. For example, LSA can be used to search for prior art given a new patent application. In this homework, we will implement a small library for simple latent semantic analysis as a practical example of the application of SVD. The ideas are very similar to PCA.\n",
      "\n",
      "We will implement a toy example of LSA to get familiar with the ideas. If you want to use LSA or similar methods for statiscal language analyis, the most efficient Python library is probably [gensim](https://radimrehurek.com/gensim/) - this also provides an online algorithm - i.e. the training information can be continuously updated. Other useful functions for processing natural language can be found in the [Natural Lnaguage Toolkit](http://www.nltk.org/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: The SVD from scipy.linalg performs a full decomposition, which is inefficient since we only need to decompose until we get the first k singluar values. If the SVD from `scipy.linalg` is too slow, please use the `sparsesvd` function from the [sparsesvd](https://pypi.python.org/pypi/sparsesvd/) package to perform SVD instead.  You can install in the usual way with \n",
      "```\n",
      "!pip install sparsesvd\n",
      "```\n",
      "\n",
      "Then import the following\n",
      "```python\n",
      "from sparsesvd import sparsesvd \n",
      "from scipy.sparse import csc_matrix \n",
      "```\n",
      "\n",
      "and use as follows\n",
      "```python\n",
      "sparsesvd(csc_matrix(M), k=10)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pip install sparsesvd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Requirement already satisfied (use --upgrade to upgrade): sparsesvd in /Users/radhika/anaconda/lib/python2.7/site-packages\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.6.0 in /Users/radhika/anaconda/lib/python2.7/site-packages (from sparsesvd)\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): cython in /Users/radhika/anaconda/lib/python2.7/site-packages (from sparsesvd)\r\n",
        "Cleaning up...\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sparsesvd import sparsesvd \n",
      "from scipy.sparse import csc_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 1 (10 points)**.  Calculating pairwise distance matrices.\n",
      "\n",
      "Suppose we want to construct a distance matrix between the rows of a matrix. For example, given the matrix \n",
      "\n",
      "```python\n",
      "M = np.array([[1,2,3],[4,5,6]])\n",
      "```\n",
      "\n",
      "the distance matrix using Euclidean distance as the measure would be\n",
      "```python\n",
      "[[ 0.000  1.414  2.828]\n",
      " [ 1.414  0.000  1.414]\n",
      " [ 2.828  1.414  0.000]] \n",
      "```\n",
      "if $M$ was a collection of column vectors.\n",
      "\n",
      "Write a function to calculate the pairwise-distance matrix given the matrix $M$ and some arbitrary distance function. Your functions should have the following signature:\n",
      "```\n",
      "def func_name(M, distance_func):\n",
      "    pass\n",
      "```\n",
      "\n",
      "0. Write a distance function for the Euclidean, squared Euclidean and cosine measures.\n",
      "1. Write the function using looping for M as a collection of row vectors.\n",
      "2. Write the function using looping for M as a collection of column vectors.\n",
      "3. Wrtie the function using broadcasting for M as a colleciton of row vectors.\n",
      "4. Write the function using broadcasting for M as a colleciton of column vectors. \n",
      "\n",
      "For 3 and 4, try to avoid using transposition (but if you get stuck, there will be no penalty for using transpoition). Check that all four functions give the same result when applied to the given matrix $M$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.array([[1,2,3],[4,5,6]])\n",
      "\n",
      "#Part1\n",
      "def euclidian_dis(a, b):\n",
      "    return np.sum((a-b)**2,-1)**0.5\n",
      "\n",
      "def squared_euclidian_dis(a, b):\n",
      "    return np.sum((a-b)**2,-1)\n",
      "\n",
      "def cosine_dis(a,b):\n",
      "    return 1-np.sum(a*b,-1)/(np.sum(a**2,-1)**0.5*np.sum(b**2,-1)**0.5)\n",
      "\n",
      "#Part2\n",
      "def row_loop(M, distance_func):\n",
      "    dis=np.zeros((M.shape[0],M.shape[0]))\n",
      "    for i in range(M.shape[0]):\n",
      "        for j in range(i,M.shape[0]):\n",
      "            dis[i,j]=distance_func(M[i,:],M[j,:])\n",
      "    dis=dis+dis.T-np.diag(dis.diagonal())\n",
      "    return dis\n",
      " \n",
      "#Part3\n",
      "def col_loop(M, distance_func):\n",
      "    n=M.shape[1]\n",
      "    dis=np.zeros((n,n))\n",
      "    for i in range(n):\n",
      "        for j in range(n):\n",
      "            dis[i,j]=distance_func(M[:,i], M[:,j])    \n",
      "    return dis\n",
      "\n",
      "#Part4\n",
      "def row_broadcast(M, distance_func):\n",
      "    dis=distance_func(M[None,:],M[:,None])\n",
      "    return dis\n",
      "\n",
      "#Part 5\n",
      "def col_broadcast(M, distance_func):\n",
      "    dis=distance_func(M.T[None,:],M.T[:,None])\n",
      "    return dis\n",
      "\n",
      "#Testing\n",
      "print \"eucildian row loop\"\n",
      "print row_loop(M, euclidian_dis)\n",
      "print \"squared eucildian row loop\"\n",
      "print row_loop(M, squared_euclidian_dis)\n",
      "print \"cosine row loop\"\n",
      "print row_loop(M, cosine_dis), '\\n'\n",
      "\n",
      "print \"eucildian row broadcast\"\n",
      "print row_broadcast(M, euclidian_dis)\n",
      "print \"squared eucildian row broadcast\"\n",
      "print row_broadcast(M, squared_euclidian_dis)\n",
      "print \"cosine row broadcast\"\n",
      "print row_broadcast(M, cosine_dis), '\\n'\n",
      "\n",
      "print \"We see that we get the same results using looping or broadcasting for each of the distances\", '\\n'\n",
      "\n",
      "print \"eucildian column loop\"\n",
      "print col_loop(M, euclidian_dis)\n",
      "print \"squared eucildian column loop\"\n",
      "print col_loop(M, squared_euclidian_dis)\n",
      "print \"cosine column loop\"\n",
      "print col_loop(M, cosine_dis), '\\n'\n",
      "\n",
      "print \"eucildian column broadcast\"\n",
      "print col_broadcast(M, euclidian_dis)\n",
      "print \"squared eucildian column broadcast\"\n",
      "print col_broadcast(M, squared_euclidian_dis)\n",
      "print \"cosine column broadcast\"\n",
      "print col_broadcast(M, cosine_dis), '\\n'\n",
      "\n",
      "print \"We see that we get the same results using looping or broadcasting for each of the distances\", '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "eucildian row loop\n",
        "[[ 0.000  5.196]\n",
        " [ 5.196  0.000]]\n",
        "squared eucildian row loop\n",
        "[[ 0.000  27.000]\n",
        " [ 27.000  0.000]]\n",
        "cosine row loop\n",
        "[[ 0.000  0.025]\n",
        " [ 0.025  0.000]] \n",
        "\n",
        "eucildian row broadcast\n",
        "[[ 0.000  5.196]\n",
        " [ 5.196  0.000]]\n",
        "squared eucildian row broadcast\n",
        "[[ 0 27]\n",
        " [27  0]]\n",
        "cosine row broadcast\n",
        "[[ 0.000  0.025]\n",
        " [ 0.025  0.000]] \n",
        "\n",
        "We see that we get the same results using looping or broadcasting for each of the distances \n",
        "\n",
        "eucildian column loop\n",
        "[[ 0.000  1.414  2.828]\n",
        " [ 1.414  0.000  1.414]\n",
        " [ 2.828  1.414  0.000]]\n",
        "squared eucildian column loop\n",
        "[[ 0.000  2.000  8.000]\n",
        " [ 2.000  0.000  2.000]\n",
        " [ 8.000  2.000  0.000]]\n",
        "cosine column loop\n",
        "[[ 0.000  0.009  0.024]\n",
        " [ 0.009 -0.000  0.003]\n",
        " [ 0.024  0.003  0.000]] \n",
        "\n",
        "eucildian column broadcast\n",
        "[[ 0.000  1.414  2.828]\n",
        " [ 1.414  0.000  1.414]\n",
        " [ 2.828  1.414  0.000]]\n",
        "squared eucildian column broadcast\n",
        "[[0 2 8]\n",
        " [2 0 2]\n",
        " [8 2 0]]\n",
        "cosine column broadcast\n",
        "[[ 0.000  0.009  0.024]\n",
        " [ 0.009 -0.000  0.003]\n",
        " [ 0.024  0.003  0.000]] \n",
        "\n",
        "We see that we get the same results using looping or broadcasting for each of the distances \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 2 (10 points)**. Write 3 functions to calculate the term frequency (tf), the inverse document frequency (idf) and the product (tf-idf). Each function should take a single argument `docs`, which is a dictionary of (key=identifier, value=dcoument text) pairs, and return an appropriately sized array. Convert '-' to ' ' (space), remove punctuation, convert text to lowercase and split on whitespace to generate a collection of terms from the dcoument text.\n",
      "\n",
      "- tf = the number of occurrences of term $i$ in document $j$\n",
      "- idf = $\\log \\frac{n}{1 + \\text{df}_i}$ where $n$ is the total number of documents and $\\text{df}_i$ is the number of documents in which term $i$ occurs.\n",
      "\n",
      "Print the table of tf-idf values for the following document collection\n",
      "\n",
      "```\n",
      "s1 = \"The quick brown fox\"\n",
      "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
      "s3 = \"The the the lazy dog elephant.\"\n",
      "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
      "\n",
      "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}\n",
      "```\n",
      "\n",
      "Note: You can use either a numpy array or pandas dataframe to store the matrix. However, we suggest using a Pnadas dataframe since that will allow you to keep track of the row (term) and column (document) names in a single object. Of course, you could also maintain a numpy matrix, a list of terms, and a list of documents separately if you prefer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "from math import log\n",
      "from pandas import DataFrame\n",
      "\n",
      "#clean up the document\n",
      "def tokenize(doc):\n",
      "    return filter(lambda c: c not in string.punctuation, doc.replace('-', ' ')).lower().split()\n",
      "\n",
      "#calculate tf\n",
      "def tfs(docs):\n",
      "    term_map = {}\n",
      "    for key, doc in docs.iteritems():\n",
      "        tokens = tokenize(doc) #cleaning\n",
      "        for token in tokens:\n",
      "            if token not in term_map: term_map[token] = {}\n",
      "            if key not in term_map[token]: term_map[token][key] = 0\n",
      "            term_map[token][key] += 1\n",
      "\n",
      "    return DataFrame(term_map).replace(to_replace='NaN',value=0).T\n",
      "\n",
      "#calculate idf\n",
      "def idf(docs):\n",
      "    term_occ = {}\n",
      "    for key, doc in docs.iteritems():\n",
      "        tokens = set(tokenize(doc))\n",
      "        for token in tokens:\n",
      "            if token not in term_occ: term_occ[token] = 0\n",
      "            term_occ[token] += 1\n",
      "\n",
      "    idf_series = {}\n",
      "    ndoc = len(docs)\n",
      "    for term, val in term_occ.iteritems():\n",
      "        idf_series[term] = log(float(ndoc)/(1+val))\n",
      "\n",
      "    return idf_series\n",
      "\n",
      "#calculate tf-idf\n",
      "def tf_idf(docs):\n",
      "    tfs_data = tfs(docs).T\n",
      "    idf_data = idf(docs)\n",
      "    \n",
      "    ordered_idf = []\n",
      "    for term in tfs_data.columns.values:\n",
      "        ordered_idf.append(idf_data[term])\n",
      "\n",
      "    return tfs_data.rmul(ordered_idf, axis='columns').fillna(value=0).T\n",
      "\n",
      "#testing\n",
      "s1 = \"The quick brown fox\"\n",
      "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
      "s3 = \"The the the lazy dog elephant.\"\n",
      "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
      "\n",
      "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}\n",
      "\n",
      "print \"tf is:\", '\\n'\n",
      "print tfs(docs), '\\n'\n",
      "\n",
      "print \"idf is:\", '\\n'\n",
      "print idf(docs), '\\n'\n",
      "\n",
      "print \"tf-idf is:\", '\\n'\n",
      "print tf_idf(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tf is: \n",
        "\n",
        "          s1  s2  s3  s4\n",
        "brown      1   1   0   0\n",
        "dog        0   0   1   1\n",
        "elephant   0   0   1   1\n",
        "fox        1   1   0   0\n",
        "jumps      0   4   0   0\n",
        "lazy       0   0   1   0\n",
        "lion       0   0   0   1\n",
        "over       0   1   0   0\n",
        "peacock    0   0   0   1\n",
        "quick      1   0   0   0\n",
        "the        1   1   3   5\n",
        "tiger      0   0   0   1 \n",
        "\n",
        "idf is: \n",
        "\n",
        "{'brown': 0.28768207245178085, 'lazy': 0.6931471805599453, 'peacock': 0.6931471805599453, 'jumps': 0.6931471805599453, 'fox': 0.28768207245178085, 'dog': 0.28768207245178085, 'tiger': 0.6931471805599453, 'lion': 0.6931471805599453, 'elephant': 0.28768207245178085, 'quick': 0.6931471805599453, 'the': -0.2231435513142097, 'over': 0.6931471805599453} \n",
        "\n",
        "tf-idf is: \n",
        "\n",
        "                s1        s2        s3        s4\n",
        "brown     0.287682  0.287682  0.000000  0.000000\n",
        "dog       0.000000  0.000000  0.287682  0.287682\n",
        "elephant  0.000000  0.000000  0.287682  0.287682\n",
        "fox       0.287682  0.287682  0.000000  0.000000\n",
        "jumps     0.000000  2.772589  0.000000  0.000000\n",
        "lazy      0.000000  0.000000  0.693147  0.000000\n",
        "lion      0.000000  0.000000  0.000000  0.693147\n",
        "over      0.000000  0.693147  0.000000  0.000000\n",
        "peacock   0.000000  0.000000  0.000000  0.693147\n",
        "quick     0.693147  0.000000  0.000000  0.000000\n",
        "the      -0.223144 -0.223144 -0.669431 -1.115718\n",
        "tiger     0.000000  0.000000  0.000000  0.693147\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 3 (10 points)**. \n",
      "\n",
      "1. Write a function that takes a matrix $M$ and an integer $k$ as arguments, and reconstructs a reduced matrix using only the $k$ largest singular values. Use the `scipy.linagl.svd` function to perform the decomposition. This is the least squares approximation to the matrix $M$ in $k$ dimensions.\n",
      "\n",
      "2. Apply the function you just wrote to the following term-frequency matrix for a set of $9$ documents using $k=2$ and print the reconstructed matrix $M'$.\n",
      "```\n",
      "M = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "```\n",
      "\n",
      "3. Calculate the pairwise correlation matrix for the original matrix M and the reconstructed matrix using $k=2$ singular values (you may use [scipy.stats.spearmanr](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) to do the calculations). Consider the fist 5 sets of documents as one group $G1$ and the last 4 as another group $G2$ (i.e. first 5 and last 4 columns). What is the average within group correlation for $G1$, $G2$ and the average cross-group correlation for G1-G2 using either $M$ or $M'$. (Do not include self-correlation in the within-group calculations.)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Part 1: function for decomposition\n",
      "def dec(M,k):\n",
      "   U,sigma,V=la.svd(M, full_matrices=False)\n",
      "   return U[:,0:k].dot(np.diag(sigma[0:k]).dot(V[0:k,:]))\n",
      "\n",
      "#Part 2: Application to given matrix\n",
      "M = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "    [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "    [0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "\n",
      "print \"Reconstructed matrix for k=2 is:\", '\\n'\n",
      "print dec(M,2), '\\n'\n",
      "\n",
      "#Part 3: Pairwise correlations\n",
      "corM=st.spearmanr(M)[0]\n",
      "print \"Pairwise correl matrix for matrix M is:\", '\\n'\n",
      "print corM, '\\n'\n",
      "\n",
      "Mp=dec(M,2)\n",
      "corMp=st.spearmanr(Mp)[0]\n",
      "print \"Pairwise correl matrix for matrix M Prime is:\", '\\n'\n",
      "print corMp, '\\n'\n",
      "\n",
      "#average within group correlations\n",
      "corG1=corM[0:5,0:5]\n",
      "corG2=corM[5:,5:]\n",
      "corG1p=corMp[0:5,0:5]\n",
      "corG2p=corMp[5:,5:]\n",
      "\n",
      "#using M\n",
      "upG1=np.triu(corG1)-np.diag(np.diag(np.triu(corG1)))\n",
      "print \"Average within group correl for G1 using M is:\",np.sum(upG1)*0.1\n",
      "\n",
      "upG2=np.triu(corG2)-np.diag(np.diag(np.triu(corG2)))\n",
      "print \"Average within group correl for G2 using M is:\",np.sum(upG2)/6\n",
      "\n",
      "corG1G2=corM[0:5,5:]\n",
      "print \"Average cross group correl for G1-G2 using M is:\",np.sum(corG1G2)*0.05\n",
      "\n",
      "#using M Prime\n",
      "upG1p=np.triu(corG1p)-np.diag(np.diag(np.triu(corG1p)))\n",
      "print \"Average within group correl for G1 using M Prime is:\",np.sum(upG1p)*0.1\n",
      "\n",
      "upG2p=np.triu(corG2p)-np.diag(np.diag(np.triu(corG2p)))\n",
      "print \"Average within group correl for G2 using M Prime is:\",np.sum(upG2p)/6\n",
      "\n",
      "corG1G2p = corMp[0:5,5:]\n",
      "print \"Average cross group correl for G1-G2 using Mprime is:\",np.sum(corG1G2p)*0.05"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reconstructed matrix for k=2 is: \n",
        "\n",
        "[[ 0.162  0.400  0.379  0.468  0.176 -0.053 -0.115 -0.159 -0.092]\n",
        " [ 0.141  0.370  0.329  0.400  0.165 -0.033 -0.071 -0.097 -0.043]\n",
        " [ 0.152  0.505  0.358  0.410  0.236  0.024  0.060  0.087  0.124]\n",
        " [ 0.258  0.841  0.606  0.697  0.392  0.033  0.083  0.122  0.187]\n",
        " [ 0.449  1.234  1.051  1.266  0.556 -0.074 -0.155 -0.210 -0.049]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.218  0.550  0.511  0.628  0.243 -0.065 -0.143 -0.197 -0.108]\n",
        " [ 0.097  0.532  0.230  0.212  0.267  0.137  0.315  0.444  0.425]\n",
        " [-0.061  0.232 -0.139 -0.266  0.145  0.240  0.546  0.767  0.664]\n",
        " [-0.065  0.335 -0.146 -0.301  0.203  0.306  0.695  0.977  0.849]\n",
        " [-0.043  0.254 -0.097 -0.208  0.152  0.221  0.503  0.707  0.616]] \n",
        "\n",
        "Pairwise correl matrix for matrix M is: \n",
        "\n",
        "[[ 1.000 -0.192  0.000  0.073 -0.333 -0.174 -0.258 -0.333 -0.333]\n",
        " [-0.192  1.000  0.000 -0.127  0.577 -0.302 -0.447 -0.577 -0.192]\n",
        " [ 0.000  0.000  1.000  0.438  0.000 -0.213 -0.316 -0.408 -0.408]\n",
        " [ 0.073 -0.127  0.438  1.000 -0.330 -0.172 -0.256 -0.330 -0.330]\n",
        " [-0.333  0.577  0.000 -0.330  1.000 -0.174 -0.258 -0.333 -0.333]\n",
        " [-0.174 -0.302 -0.213 -0.172 -0.174  1.000  0.674  0.522 -0.174]\n",
        " [-0.258 -0.447 -0.316 -0.256 -0.258  0.674  1.000  0.775  0.258]\n",
        " [-0.333 -0.577 -0.408 -0.330 -0.333  0.522  0.775  1.000  0.556]\n",
        " [-0.333 -0.192 -0.408 -0.330 -0.333 -0.174  0.258  0.556  1.000]] \n",
        "\n",
        "Pairwise correl matrix for matrix M Prime is: \n",
        "\n",
        "[[ 1.000  0.846  1.000  1.000  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 0.846  1.000  0.846  0.846  0.972 -0.557 -0.557 -0.557 -0.480]\n",
        " [ 1.000  0.846  1.000  1.000  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 1.000  0.846  1.000  1.000  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 0.719  0.972  0.719  0.719  1.000 -0.389 -0.389 -0.389 -0.298]\n",
        " [-0.837 -0.557 -0.837 -0.837 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.837 -0.557 -0.837 -0.837 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.837 -0.557 -0.837 -0.837 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.802 -0.480 -0.802 -0.802 -0.298  0.979  0.979  0.979  1.000]] \n",
        "\n",
        "Average within group correl for G1 using M is: 0.0105776866299\n",
        "Average within group correl for G2 using M is: 0.43511771482\n",
        "Average cross group correl for G1-G2 using M is: -0.307562188906\n",
        "Average within group correl for G1 using M Prime is: 0.866666666667\n",
        "Average within group correl for G2 using M Prime is: 0.98951048951\n",
        "Average cross group correl for G1-G2 using Mprime is: -0.677759358117\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 4 (20 points)**. Clustering with LSA\n",
      "\n",
      "1. Begin by loading a pubmed database of selected article titles using 'cPickle'. With the following:\n",
      "```import cPickle\n",
      "docs = cPickle.load(open('pubmed.pic'))```\n",
      "\n",
      "    Create a tf-idf matrix for every term that appears at least once in any of the documents. What is the shape of the tf-idf matrix? \n",
      "\n",
      "2. Perform SVD on the tf-idf matrix to obtain $U \\Sigma V^T$ (often written as $T \\Sigma D^T$ in this context with $T$ representing the terms and $D$ representing the documents). If we set all but the top $k$ singular values to 0, the reconstructed matrix is essentially $U_k \\Sigma_k V_k^T$, where $U_k$ is $m \\times k$, $\\Sigma_k$ is $k \\times k$ and $V_k^T$ is $k \\times n$. Terms in this reduced space are represented by $U_k \\Sigma_k$ and documents by $\\Sigma_k V^T_k$. Reconstruct the matrix using the first $k=10$ singular values.\n",
      "\n",
      "3. Use agglomerative hierachical clustering with complete linkage to plot a dendrogram and comment on the likely number of  document clusters with $k = 100$. Use the dendrogram function from [SciPy ](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.cluster.hierarchy.dendrogram.html).\n",
      "\n",
      "4. Determine how similar each of the original documents is to the new document `mystery.txt`. Since $A = U \\Sigma V^T$, we also have $V = A^T U S^{-1}$ using orthogonality and the rule for transposing matrix products. This suggests that in order to map the new document to the same concept space, first find the tf-idf vector $v$ for the new document - this must contain all (and only) the terms present in the existing tf-idx matrix. Then the query vector $q$ is given by $v^T U_k \\Sigma_k^{-1}$. Find the 10 documents most similar to the new document and the 10 most dissimilar. \n",
      "\n",
      "5. Many documents often have some boilerplate material such as organization information, Copyright, etc. at the front or back of the document. Does it matter that the front and back matter of each document is essentially identical for either LSA-based clustering (part 3) or information retrieval (part 4)? Why or why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.linalg import svd\n",
      "\n",
      "#Part 1\n",
      "import cPickle\n",
      "docs = cPickle.load(open('pubmed.pic'))\n",
      "\n",
      "m=tf_idf(docs)\n",
      "print \"Not printing the tf-idf matrix as it is very big to be displayed\", '\\n'\n",
      "print \"The shape of this is matrix is:\", m.shape, '\\n'\n",
      "\n",
      "#Part 2\n",
      "print \"Recontsructed matrix using k=10 is:\", '\\n'\n",
      "print dec(m,10) #dec is the function defined in question 3 to decompose"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Part 3: dendogram\n",
      "from scipy.cluster.hierarchy import linkage, dendrogram\n",
      "\n",
      "decom=dec(m.T,100)\n",
      "dm=row_loop(decom,cosine_dis)\n",
      "\n",
      "fig,axes=plt.subplots(figsize=(5,30))\n",
      "dend=dendrogram(linkage(dm,method='complete'),orientation='right',labels= [x[:50] for x in m.keys()])\n",
      "type(dend)\n",
      "\n",
      "print \"The likely number of clusters with k=100 from this dendogram is 13 as there are 13 different\\\n",
      " colored clusters in the end\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Part 4\n",
      "\n",
      "#Cleaning up mystery\n",
      "newdoc=open('mystery.txt').read()\n",
      "oldWords = idf(docs).keys()\n",
      "newdoc = tokenize(newdoc)\n",
      "#Making sure we include only the words there in existing tf-idf\n",
      "newDoc = [x for x in newdoc if x in oldWords]\n",
      "newDoc=' '.join(newDoc)\n",
      "ndoc={\"mystery\":newDoc}\n",
      "final=dict(docs.items()+ndoc.items())\n",
      "inter=tf_idf(final)\n",
      "mys=inter[\"mystery\"]\n",
      "\n",
      "#storing doc names\n",
      "doclist = list(inter.columns.values)\n",
      "\n",
      "#sparsesvd using k=100 from part 3\n",
      "U,s,V = sparsesvd(csc_matrix(tf_idf(docs)), 100)\n",
      "\n",
      "#making concept matrix\n",
      "cc = np.linalg.inv(np.diag(s)).dot(U.dot(tf_idf(docs)))\n",
      "\n",
      "#making query matrix\n",
      "q=mys.T.dot(U.T.dot(la.inv(np.diag(s))))[:,np.newaxis]\n",
      "\n",
      "#final conceot matrix\n",
      "c=np.hstack([cc, q])\n",
      "\n",
      "#calculating cosine similarity\n",
      "csym=1-col_loop(c,cosine_dis)\n",
      "\n",
      "#taking the last column corresponsing to mystery\n",
      "final_sym=csym[-1,:]\n",
      "\n",
      "#Most similar\n",
      "mostSimIndex=sorted(range(len(final_sym)),key=lambda i: final_sym[i])[-11:-1]\n",
      "mostSim=pd.DataFrame([doclist[i] for i in mostSimIndex], columns=[\"Documents\"])\n",
      "print \"10 most similar documents to mystery are:\", '\\n'\n",
      "print mostSim.iloc[::-1], '\\n'\n",
      "\n",
      "#Least similar\n",
      "leastSimIndex=sorted(range(len(final_sym)),key=lambda i: final_sym[i])[:10]\n",
      "leastSim=pd.DataFrame([doclist[i] for i in leastSimIndex],columns=[\"Documents\"])\n",
      "print \"10 least similar documents are:\", '\\n'\n",
      "print leastSim, '\\n'\n",
      "\n",
      "#Part 5\n",
      "print \"Ans. part 5. The boilerplate material is also used in calculating the tf-idf values and the chances of \\\n",
      "occurences of some words are very high in the bolier plate material. So they will technically have \\\n",
      "an effect on clustering and retrieval but since the boilerplate material is very small compared to the\\\n",
      " full document the effect will be very insignificant\", '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notes on the Pubmed articles\n",
      "----\n",
      "\n",
      "These were downloaded with the following script.\n",
      "\n",
      "```python\n",
      "from Bio import Entrez, Medline\n",
      "Entrez.email = \"YOUR EMAIL HERE\"\n",
      "import cPickle\n",
      "\n",
      "try:\n",
      "    docs = cPickle.load(open('pubmed.pic'))\n",
      "except Exception, e:\n",
      "    print e\n",
      "\n",
      "    docs = {}\n",
      "    for term in ['plasmodium', 'diabetes', 'asthma', 'cytometry']:\n",
      "        handle = Entrez.esearch(db=\"pubmed\", term=term, retmax=50)\n",
      "        result = Entrez.read(handle)\n",
      "        handle.close()\n",
      "        idlist = result[\"IdList\"]\n",
      "        handle2 = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode=\"text\")\n",
      "        result2 = Medline.parse(handle2)\n",
      "        for record in result2:\n",
      "            title = record.get(\"TI\", None)\n",
      "            abstract = record.get(\"AB\", None)\n",
      "            if title is None or abstract is None:\n",
      "                continue\n",
      "            docs[title] = '\\n'.join([title, abstract])\n",
      "            print title\n",
      "        handle2.close()\n",
      "    cPickle.dump(docs, open('pubmed.pic', 'w'))\n",
      "docs.values()\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}